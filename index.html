<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yug D Oswal | Portfolio</title>
  
  <meta name="author" content="Yug D Oswal">
  <meta name="description" content="I am Yug Oswal, a 3rd year UG CS student at VIT Vellore. I like mechanistic interpretability and the theory of deep learning.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="index,follow">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
	      <h1 style="text-align:center">Yug D Oswal</h1>
	      <p>I am a third year UG CS student at VIT Vellore. I'm most recently working on a <a href="https://arxiv.org/abs/2405.04459">paper</a> we're soon going to be submitting to ICML 2025. I specialize in AI/ML, and am also proficient in Data Science, Web Dev, and App Dev (Flutter). I do work in the knowledge representation, architectures, and the theory of deep learning - novel activation functions, loss functions, optimizers, schedulers. I'm most interested in mechanistic interpretability and the theory of deep learning.
              </p>
	      <p>Personally, I enjoy dancing, music, and reading novels. I was an abacus champion during my school years. I like thinking up crazy theories for neural nets sometimes.
              </p>
              <p style="text-align:center">
                <a href="mailto:yoswal071@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Yug's Resume - Final.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=e8SUSSEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/yug-d-oswal-082652259/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Yug_Oswal.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Yug_Oswal.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Machine Learning Research</h2>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/mistral_circles.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2405.14860">
                <papertitle>Not All Language Model Features Are Linear</papertitle>
              </a>
              <br>
              <a href="https://www.joshengels.com/">Joshua Engels</a>,
              <strong>Isaac Liao</strong>,
              <a href="https://ericjmichaud.com/">Eric J. Michaud</a>,
              <a href="https://wesg.me/">Wes Gurnee</a>,
              <a href="https://space.mit.edu/home/tegmark/">Max Tegmark</a>
              <br>
              <a href="https://arxiv.org/abs/2405.14860">arXiv</a>, 2024
              <p></p>
              <p>
	      When large language models do modular addition, the numbers are stored in a circle.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/L1_norm_network.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.03051">
                <papertitle>Generating Interpretable Networks using Hypernetworks</papertitle>
              </a>
              <br>
              <strong>Isaac Liao</strong>,
              <a href="https://kindxiaoming.github.io/">Ziming Liu</a>,
              <a href="https://space.mit.edu/home/tegmark/">Max Tegmark</a>
              <br>
              <a href="https://openreview.net/forum?id=Ns2X7Azudy">arXiv</a>, 2023
              <p></p>
              <p>
	      When we generatively model a neural network's weights, we tend to generate weights that are smartly arranged.<!--Introduction of a novel hypernetwork architecture for generative modeling of neural network weights. Architecture is a merge of Pareto hypernetworks, hierarchical VAEs, and graph transformers. Reverse-engineering of learned algorithms, with algorithmic phase transitions using order parameters and visualization via force-directed graph drawing.-->
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/LODO.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=Ns2X7Azudy">
                <papertitle>Learning to Optimize Quasi-Newton Methods</papertitle>
              </a>
              <br>
              <strong>Isaac Liao</strong>,
              <a href="http://super-ms.mit.edu/rumen.html">Rumen R. Dangovski</a>,
              <a href="https://www.jakobfoerster.com/">Jakob N. Foerster</a>,
              <a href="http://www.mit.edu/~soljacic/marin.html">Marin Solja&#269;i&#263;</a>
              <br>
              <a href="https://openreview.net/forum?id=Ns2X7Azudy">TMLR</a>, 2023
              <p></p>
              <p>
	      We can feed gradients as input into a linear neural network, get a step as an output, and train the network to perform optimization, during the optimization.<!--Introduces a novel machine learning optimization algorithm which blends learn to optimize (L2O) meta-learning techniques with quasi-Newton optimization methods using sparse neural networks. Theoretical results regarding convex and nonconvex stochastic convergence and sparse neural network expressiveness, with experimental support.-->
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/MIPS_flowchart.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2402.05110">
                <papertitle>Opening the AI Black Box: Program Synthesis via Mechanistic Interpretability</papertitle>
              </a>
              <br>
              <a href="https://ericjmichaud.com/">Eric J. Michaud</a>,
              <strong>Isaac Liao</strong>,
              <a href="https://www.vedanglad.com/">Vedang Lad</a>,
              <a href="https://kindxiaoming.github.io/">Ziming Liu</a>,
              Anish Mudide,
	      Chloe Loughridge,
              <a href="https://carlguo.com/">Zifan Carl Guo</a>,
	      Tara Rezaei Kheirkhah,
	      Mateja Vukeli&#263,
              <a href="https://space.mit.edu/home/tegmark/">Max Tegmark</a>
              <br>
              <a href="https://arxiv.org/abs/2402.05110">arXiv</a>, 2024
              <p></p>
              <p>
	      We auto-convert RNNs into interpretable python code equivalents, for model verification.<!--Novel benchmark for automated mechanistic interpretation of recurrent neural networks. Novel methods for conversion of learned recurrent neural networks into interpretable python code equivalents, for model verification.-->
              </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Research-like Class Projects</h2>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/SVD_statistics.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/6_7830_Project.pdf">
                <papertitle>Bayesian Recommender Systems</papertitle>
              </a>
              <br>
              <strong>Isaac Liao</strong>
              <br>
	      <a href="https://tamarabroderick.com/course_6_7830_2023_spring.html">6.7830 Bayesian Modeling and Inference</a>
              <p></p>
              <p>
	      We improve targeted movie recommendation systems by 2%, by using Bayesianism to add uncertainty into SVD-based large matrix completion algorithms.<!--Targeted movie recommendation systems using the Low-Rank Approximation with Alternating Least Squares method of large matrix completion. Reinterpretation of the algorithm in the Bayesian formulation, and an extension of the resulting Bayesian model to allow for uncertainty in user preferences and movie characteristics. Experiments to show that this improves recommendation accuracy.-->
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/fanout_tree.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/6_860_Project.pdf">
                <papertitle>Parameter-Efficient Approximation by Exploitation of Sparsity</papertitle>
              </a>
              <br>
              <strong>Isaac Liao</strong>
              <br>
	      <a href="https://cbmm.mit.edu/9-520">6.7910 Statistical Learning Theory</a>
              <p></p>
              <p>
	      This sparse neural network can imitate almost all other neural network architectures that have about the same amount of parameters.<!--Exploration of the expressiveness properties of sparse neural network architectures. Extensions of theorems about the ability of sparse architectures to replicate the behavior of any other sparse architecture. Experimental evaluation of the ability of sparse architectures to perform compositionally sparse linear operations.-->
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/entropy_codes.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/6_819_Project.pdf">
                <papertitle>Differential Entropy Codes for Trained Image Compression</papertitle>
              </a>
              <br>
              <strong>Isaac Liao</strong>
              <br>
	      <a href="http://6.8300.csail.mit.edu/sp21/">6.819 Advances in Computer Vision</a>
              <p></p>
              <p>
	      Reinvented most of the framework that powers VAEs and BNNs, from the lens of image compression.<!--Treatment of variational inference from the point of view of information compression, with application to images. Ideation, refinement, theoretical analysis, and empirical testing of ELBO maximization-based lossless image compression schemes resembling VAEs and BNNs. Reparameterization trick, hierarchical depth, and KL annealing schedules, and rejection sampling all independently reinvented without prior knowledge of existing variational inference techniques.-->
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/wigner_surmise.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/8_06_Project.pdf">
                <papertitle>A Perturbative Approach to Random Matrix Spectra</papertitle>
              </a>
              <br>
              <strong>Isaac Liao</strong>
              <br>
	      <a href="https://ocw.mit.edu/courses/8-06-quantum-physics-iii-spring-2018/">8.06 Quantum Physics III</a>
              <p></p>
              <p>
	      We rederive the spectrum of random (Gaussian) Hermitian matrices.<!--Rederivation of joint eigenvalue distribution of random Hermitian matrices, using a combination of second-order quantum perturbation theory, Metropolis-Hastings, and Brownian motion. Rederivation of Wigner semicircle law. Connections to chaotic quantum billiards and application to emission spectra of quantum dots.-->
              </p>
            </td>
          </tr>
        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <h2>Education Research</h2>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>

            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/problem_generation.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
	      <papertitle>Utility Teaching Assistant for MIT 8.01 Classical Mechanics I</papertitle></a>
              <p>
	      We use large language models to generate physics problems to help teach ~700 students. Resulted in publishing "<a href="https://pubs.aip.org/aapt/pte/article/62/7/595/3313859/Streamlining-Physics-Problem-Generation-to-Support"><papertitle>Streamlining Physics Problem Generation to Support Physics Teachers in Using Generative Artificial Intelligence</papertitle></a>" (<a href="https://www.shamseladawy.com/">Shams El-Adawy</a>, <strong>Isaac Liao</strong>, <a href="https://www.vedanglad.com/">Vedang Lad</a>, <a href="https://scholar.google.com/citations?hl=en&user=NKYiX0QAAAAJ&view_op=list_works&sortby=pubdate">Mohamed Abdelhafez</a>, <a href="https://physics.mit.edu/physics-directory/peter-dourmashkin/">Peter Dourmashkin</a>) in <a href="https://pubs.aip.org/aapt/pte/article/62/7/595/3313859/Streamlining-Physics-Problem-Generation-to-Support">The Physics Teacher</a>, 2024.<!--Novel methods for applying large language models to assist in teaching physics to ~700 students. Large language models for answering student questions and generating physics problems used for teaching. Drafting of manuscripts for publication in journals on physics education and AI in education. Course website maintenance. Office hours, grading, exam proctoring.-->
              </p>
            </td>
          </tr>
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <h2>Miscellaneous</h2>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>

            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/chain.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://battlecode.org/assets/files/postmortem-2021-wololo.pdf">
		      <papertitle>Swarm Intelligence for MIT Battlecode AI Programming Competition</papertitle></a>
              <p></p>
              <p>
	      I placed seventh worldwide in the <a href="https://www.battlecode.org/">Battlecode</a> 2021 Game AI programming competition, and this is my strategy report.<!--Summary report for my strategy in the <a href="https://www.battlecode.org/">Battlecode</a> 2021 Game AI programming competition, in which I got 7th place worldwide.-->
              </p>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                This website was produced from <a href="https://github.com/jonbarron/jonbarron_website">a template</a> made by Jon Barron.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
